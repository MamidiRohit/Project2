# Project 2
Student: Yuxuan Qian (A20484572)

## Boosting Trees
* __What does the model you have implemented do and when should it be used?__<br />
The gradient boosting tree I had implemented is one of the machine learning methods that can predict the regression. Since the gradient boosting algorithm trains data by combining several weak learners, I first created the `DcisionTreeRegressor` class which is applied to predict the regression data by decision tree to support a learner that the gradient boosting algorithm can integrate. Thus, the model will be utilized if we want to predict the data for linear regression.
  
* __How did you test your model to determine if it is working reasonably correctly?__<br />
I tested my model with 4 test cases. The first three cases are randomly generated regression data, and the last is the data about US housing prices. All data were first split into 80% training dataset and 20% test dataset. After importing the training data into the model, we can predict the by inputting x-testing data into the trained model. Finally, I measured the performance of the model by r square and the plot of the actual vs the prediction. The r square in all tests is more than 0.85, and the plot is roughly a line through the origin. Those show the model predicted well.
  
* __What parameters have you exposed to users of your implementation in order to tune performance? (Also perhaps provide some basic usage examples.)__<br />
`GradientBoostingTree(n_estimators=100, rate=0.1, max_depth=3, min_samples_split=2, tol=10)`<br/>
***parameter***<br/>
`n_estimators`: the number of estimators in the gradient boosting algorithm. An estimator is a decision tree that is generated by `DcisionTreeRegressor` <br/>
`rate`: the learning rate <br/>
`max depth`: the maximum depth of the tree. The value is inputted to `DcisionTreeRegressor` to control the maximum depth of an estimator <br/>
`min_samples_split`: the splitting requirement of minimum samples and is imported into `DcisionTreeRegressor`<br/>
`tol`: tolerance. It is an `int` value and controls the early stopping. For each new estimator, the model calculates the loss (mean square error) by the previous residual and predicted residual. If the loss is less than the current best loss, i.e. the model is still improving, the model will update the optimal loss and reset "tol" to the parameter we set. Otherwise, `tol` is decremented by 1 and an early stop occurs when `tol` is 0.<br/><br/>
***function***<br/>
`fit(self, X, Y)`<br/>
Fit model.<br/>
_parameter_: <br/>
`X`: x train data<br/>
`Y`: y train data<br/><br/>
`predict(X)`<br/>
Predict using gradient boosting. <br/>
_parameter_: <br/>
`X`: x test data<br/><br/>
_Usage Example:_ <br/>
1. Download the file called `Gradient_Dcision_Tree.py` <br/>
2. When using, use the following codes: <br/>
```
from Gradient_Dcision_Tree import GradientBoostingTree

x_train = 'your x training dataset'
y_train = 'your y training dataset'

x_test = 'your x test dataset'
y_test = 'your y test dataset'

model = GradientBoostingTree(n_estimators=100, rate=0.1, max_depth=3, min_samples_split=2, tol=10)
model.fit(x_train, y_train)

model.predict(x_test)
```
Also, you can use it without the parameters:<br/>
```
from Gradient_Dcision_Tree import GradientBoostingTree

x_train = 'your x training dataset'
y_train = 'your y training dataset'

x_test = 'your x test dataset'
y_test = 'your y test dataset'

model = GradientBoostingTree()
model.fit(x_train, y_train)

model.predict(x_test)
```

* __Are there specific inputs that your implementation has trouble with? Given more time, could you work around these or is it fundamental?__<br />
Test case 4 predicted the value of US housing prices. The model spent the most time training (4m 23s). The reasons are not only that the data is large (total is 5000 and train data is 4000), but also that the model needs more time to build an estimator (decision tree) and combine multiple estimators. Although I think there is still space for improvement in my code to save time, it still reflects that one of the disadvantages of the gradient boosting algorithm is that processing large datasets is time-consuming. If I have more time, I believe that I will find improving ways to save time in the gradient boosting algorithm.
