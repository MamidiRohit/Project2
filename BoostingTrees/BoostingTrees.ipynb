{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a13d3818-8ab3-4dcf-9629-6e5076cd20f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0092\n",
      "Predictions for new data: [1.8585509  4.32879154 2.16317274 1.86296403 4.3325373  2.51684381\n",
      " 2.13825765 3.48771721 1.81816335 2.06598085]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTreeRegressor:\n",
    "    \"\"\"\n",
    "    A simple decision tree regressor for fitting residuals.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_depth=3):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def _split(self, X, y):\n",
    "        \"\"\"\n",
    "        Find the best split for a dataset.\n",
    "        \"\"\"\n",
    "        best_split = {\"feature\": None, \"threshold\": None, \"loss\": float(\"inf\")}\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        for feature in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                \n",
    "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "                    continue\n",
    "                \n",
    "                left_residuals = y[left_mask]\n",
    "                right_residuals = y[right_mask]\n",
    "                \n",
    "                # Mean squared error as loss\n",
    "                loss = (\n",
    "                    np.sum((left_residuals - np.mean(left_residuals)) ** 2) +\n",
    "                    np.sum((right_residuals - np.mean(right_residuals)) ** 2)\n",
    "                )\n",
    "                \n",
    "                if loss < best_split[\"loss\"]:\n",
    "                    best_split = {\n",
    "                        \"feature\": feature,\n",
    "                        \"threshold\": threshold,\n",
    "                        \"loss\": loss,\n",
    "                        \"left_mask\": left_mask,\n",
    "                        \"right_mask\": right_mask,\n",
    "                    }\n",
    "        \n",
    "        return best_split\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        \"\"\"\n",
    "        Recursively build the decision tree.\n",
    "        \"\"\"\n",
    "        if depth >= self.max_depth or len(set(y)) == 1:\n",
    "            return {\"value\": np.mean(y)}\n",
    "\n",
    "        split = self._split(X, y)\n",
    "        if split[\"feature\"] is None:\n",
    "            return {\"value\": np.mean(y)}\n",
    "\n",
    "        left_tree = self._build_tree(X[split[\"left_mask\"]], y[split[\"left_mask\"]], depth + 1)\n",
    "        right_tree = self._build_tree(X[split[\"right_mask\"]], y[split[\"right_mask\"]], depth + 1)\n",
    "\n",
    "        return {\n",
    "            \"feature\": split[\"feature\"],\n",
    "            \"threshold\": split[\"threshold\"],\n",
    "            \"left\": left_tree,\n",
    "            \"right\": right_tree,\n",
    "        }\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, 0)\n",
    "\n",
    "    def _predict_one(self, x, tree):\n",
    "        \"\"\"\n",
    "        Predict a single sample using the tree.\n",
    "        \"\"\"\n",
    "        if \"value\" in tree:\n",
    "            return tree[\"value\"]\n",
    "        \n",
    "        feature = tree[\"feature\"]\n",
    "        threshold = tree[\"threshold\"]\n",
    "\n",
    "        if x[feature] <= threshold:\n",
    "            return self._predict_one(x, tree[\"left\"])\n",
    "        else:\n",
    "            return self._predict_one(x, tree[\"right\"])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_one(x, self.tree) for x in X])\n",
    "\n",
    "\n",
    "class GradientBoostingTree:\n",
    "    \"\"\"\n",
    "    Gradient Boosting Tree implementation with explicit gamma calculation.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3, loss=\"squared_error\"):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.init_prediction = None\n",
    "        self.loss = loss\n",
    "\n",
    "    def _gradient(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        Compute the gradient of the loss function.\n",
    "        \"\"\"\n",
    "        if self.loss == \"squared_error\":\n",
    "            return y - y_pred\n",
    "        raise ValueError(\"Unsupported loss function\")\n",
    "\n",
    "    def _gamma(self, residuals, region):\n",
    "        \"\"\"\n",
    "        Compute the optimal gamma for a region as per Equation (10.30).\n",
    "        \"\"\"\n",
    "        return np.mean(residuals[region])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the gradient boosting tree model.\n",
    "        \"\"\"\n",
    "        self.init_prediction = np.mean(y)  # Start with the mean prediction\n",
    "        predictions = np.full_like(y, self.init_prediction, dtype=np.float64)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute residuals (negative gradients)\n",
    "            residuals = self._gradient(y, predictions)\n",
    "\n",
    "            # Train a decision tree on residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            # Update predictions with the tree's contribution\n",
    "            tree_predictions = tree.predict(X)\n",
    "\n",
    "            for region in np.unique(tree_predictions):\n",
    "                mask = tree_predictions == region\n",
    "                gamma = self._gamma(residuals, mask)\n",
    "                predictions[mask] += self.learning_rate * gamma\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict target values for input data X.\n",
    "        \"\"\"\n",
    "        predictions = np.full((X.shape[0],), self.init_prediction, dtype=np.float64)\n",
    "\n",
    "        for tree in self.trees:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Import necessary libraries\n",
    "    import numpy as np\n",
    "\n",
    "    # Generate synthetic regression data\n",
    "    def make_synthetic_regression(n_samples=100, n_features=7, noise=0.1, random_state=42):\n",
    "        np.random.seed(random_state)\n",
    "        X = np.random.rand(n_samples, n_features)  # Features: random values in [0, 1]\n",
    "        coefficients = np.random.rand(n_features)  # Random coefficients for linear relation\n",
    "        y = X @ coefficients + noise * np.random.randn(n_samples)  # Linear relationship + noise\n",
    "        return X, y\n",
    "\n",
    "    # Compute mean squared error manually\n",
    "    def mean_squared_error(y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    # Generate data\n",
    "    X, y = make_synthetic_regression(n_samples=100, n_features=7, noise=0.1, random_state=42)\n",
    "    y = y / np.std(y)  # Normalize target for simplicity\n",
    "\n",
    "    # Train Gradient Boosting Tree\n",
    "    model = GradientBoostingTree(n_estimators=50, learning_rate=0.1, max_depth=3, loss=\"squared_error\")\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Predict\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    # Evaluate\n",
    "    mse = mean_squared_error(y, predictions)\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "    print(\"Predictions for new data:\", predictions[:10])  # Display first 10 predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6d74fbd-31d7-42d2-a952-3db6ef81b13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Data: 0.1252\n",
      "Predictions (original scale): [1115218.35094651 1422022.74149671 1296651.00128986 1335160.96679622\n",
      " 1420706.07475337 1230924.16022955  929422.37401399 1163514.79813095\n",
      " 1304559.25219602 1125881.6236073 ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "dataset1 = pd.read_csv(\"D:/IIT/Fall 2024/ML/Project 2/housing.csv\")\n",
    "\n",
    "# Drop irrelevant column and define features and target\n",
    "X = dataset1.drop(['Price', 'Address'], axis=1)\n",
    "y = dataset1['Price']\n",
    "\n",
    "# Normalize features\n",
    "X = (X - X.mean()) / X.std()\n",
    "\n",
    "# Normalize target variable\n",
    "y_mean = y.mean()\n",
    "y_std = y.std()\n",
    "y = (y - y_mean) / y_std\n",
    "\n",
    "# Manual Train-Test Split (80% Train, 20% Test)\n",
    "n_samples = X.shape[0]\n",
    "split_ratio = 0.8\n",
    "split_index = int(n_samples * split_ratio)\n",
    "\n",
    "indices = np.arange(n_samples)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices = indices[:split_index]\n",
    "test_indices = indices[split_index:]\n",
    "\n",
    "X_train, X_test = X.values[train_indices], X.values[test_indices]\n",
    "y_train, y_test = y.values[train_indices], y.values[test_indices]\n",
    "\n",
    "# Train Gradient Boosting Tree\n",
    "model = GradientBoostingTree(n_estimators=50, learning_rate=0.1, max_depth=3, loss=\"squared_error\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on Test Data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = np.mean((y_test - predictions) ** 2)\n",
    "print(f\"Mean Squared Error on Test Data: {mse:.4f}\")\n",
    "\n",
    "# Rescale Predictions Back to Original Scale\n",
    "predictions_original_scale = predictions * y_std + y_mean\n",
    "print(\"Predictions (original scale):\", predictions_original_scale[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b44a7fcc-64b5-4944-9a4d-da51c21f73f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.4848\n",
      "Predictions for the dataset: [16.45327894 16.45327894 16.45327894 16.45327894 20.74959815 20.74959815\n",
      " 20.74959815 20.74959815 19.80121047 19.80121047]\n"
     ]
    }
   ],
   "source": [
    "#Dataset2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load Energy Efficiency dataset\n",
    "file_path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\"\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Features and Targets\n",
    "X = data.iloc[:, :-2].values  # All columns except the last two (Heating and Cooling loads)\n",
    "y = data.iloc[:, -2].values  # Target: Heating load\n",
    "\n",
    "# Normalize the features (optional but recommended for Gradient Boosting)\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# Train Gradient Boosting Tree\n",
    "model = GradientBoostingTree(n_estimators=50, learning_rate=0.1, max_depth=3, loss=\"squared_error\")\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Evaluate\n",
    "mse = np.mean((y - predictions) ** 2)  # Mean Squared Error\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "# Print predictions for new data\n",
    "print(\"Predictions for the dataset:\", predictions[:10])  # Show the first 10 predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7560de01-82f1-4299-a140-0e3d9b60fa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Data: 0.1689\n",
      "Predictions (original scale): [10080.48355121 13895.33265983 18348.4838738  16775.67564369\n",
      " 13850.04125789  2991.89635303  4363.66200959 10698.70506793\n",
      " 18509.28425974  6344.20381705]\n"
     ]
    }
   ],
   "source": [
    "#Dataset 3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load Medical Cost dataset\n",
    "file_path = \"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "data = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "# Features and Target\n",
    "X = data.drop('charges', axis=1).values  # Convert features to NumPy array\n",
    "y = data['charges'].values  # Convert target to NumPy array\n",
    "\n",
    "# Normalize the target variable (y)\n",
    "y_mean = np.mean(y)\n",
    "y_std = np.std(y)\n",
    "y = (y - y_mean) / y_std  # Normalize\n",
    "\n",
    "# Manual Train-Test Split (80% Train, 20% Test)\n",
    "n_samples = X.shape[0]\n",
    "split_ratio = 0.8  # 80% train, 20% test\n",
    "split_index = int(n_samples * split_ratio)\n",
    "\n",
    "# Shuffle indices\n",
    "indices = np.arange(n_samples)\n",
    "np.random.seed(42)  # For reproducibility\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Split the data\n",
    "train_indices = indices[:split_index]\n",
    "test_indices = indices[split_index:]\n",
    "\n",
    "X_train, X_test = X[train_indices], X[test_indices]\n",
    "y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "# Train Gradient Boosting Tree\n",
    "model = GradientBoostingTree(n_estimators=50, learning_rate=0.1, max_depth=3, loss=\"squared_error\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on Test Data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = np.mean((y_test - predictions) ** 2)\n",
    "print(f\"Mean Squared Error on Test Data: {mse:.4f}\")\n",
    "\n",
    "# Rescale Predictions Back to Original Scale\n",
    "predictions_original_scale = predictions * y_std + y_mean\n",
    "\n",
    "# Print example predictions\n",
    "print(\"Predictions (original scale):\", predictions_original_scale[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16709f75-fb97-4e4a-8fae-220d930ec0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pooja\\AppData\\Local\\Temp\\ipykernel_11216\\1415650776.py:6: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv(file_path, delim_whitespace=True, names=columns, na_values=\"?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0080\n",
      "Predictions for test data: [4.12896439 5.98399739 4.00468102 4.00468102 4.00468102 4.00468102\n",
      " 4.00468102 4.00468102 4.25208439 4.00468102]\n"
     ]
    }
   ],
   "source": [
    "#Dataset 4\n",
    "# Load and Process Auto MPG Dataset\n",
    "file_path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n",
    "columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin']\n",
    "\n",
    "data = pd.read_csv(file_path, delim_whitespace=True, names=columns, na_values=\"?\")\n",
    "\n",
    "# Handle missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# One-hot encode the 'origin' column\n",
    "data = pd.get_dummies(data, columns=['origin'], drop_first=True)\n",
    "\n",
    "# Features and Target\n",
    "X = data.drop('mpg', axis=1).values  # Convert to NumPy array\n",
    "y = data['mpg'].values\n",
    "\n",
    "# Manual Train-Test Split (80-20 split)\n",
    "n_samples = X.shape[0]\n",
    "split_index = int(n_samples * 0.8)\n",
    "indices = np.arange(n_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices = indices[:split_index]\n",
    "test_indices = indices[split_index:]\n",
    "\n",
    "X_train, X_test = X[train_indices], X[split_index:]\n",
    "y_train, y_test = y[train_indices], y[split_index:]\n",
    "\n",
    "# Train Gradient Boosting Tree\n",
    "model = GradientBoostingTree(n_estimators=50, learning_rate=0.1, max_depth=3, loss=\"squared_error\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on Test Data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = np.mean((y_test - predictions) ** 2)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "# Print some predictions\n",
    "print(\"Predictions for test data:\", predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cfbd2b-ec52-459c-ab94-6815fcb6b048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
