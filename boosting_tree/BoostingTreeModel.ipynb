{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "class MyMSE:\n",
    "    @staticmethod\n",
    "    def calculate(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculates the Mean Squared Error between true and predicted values.\n",
    "        Args: y_true (np.array): Ground truth labels. | y_pred (np.array): Predicted labels.\n",
    "        Returns: float: The mean squared error between y_true and y_pred.\n",
    "        \"\"\"\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "class MyRSquared:\n",
    "    @staticmethod\n",
    "    def calculate(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculates the R-squared metric for regression.\n",
    "        Args: y_true (np.array): Ground truth labels. | y_pred (np.array): Predicted labels.\n",
    "        Returns: float: The R-squared value between y_true and y_pred.\n",
    "        \"\"\"\n",
    "        ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "class BoostingTreeModel:\n",
    "    def __init__(self, num_trees=20, learning_rate=0.1, max_depth=3, tol=1e-5, subsample=0.5):\n",
    "        \"\"\"\n",
    "        Initializes a BoostingTreeModel object.\n",
    "        Args:\n",
    "            num_trees (int, optional): Number of trees to use in the ensemble. Defaults to 20.\n",
    "            learning_rate (float, optional): Learning rate for the boosting process. Defaults to 0.1.\n",
    "            max_depth (int, optional): Maximum depth of individual trees. Defaults to 3.\n",
    "            tol (float, optional): Tolerance for early stopping. Defaults to 1e-5.\n",
    "            subsample (float, optional): Fraction of data to use for training each tree (bagging). Defaults to 0.5.\n",
    "        \"\"\"\n",
    "        self.num_trees = num_trees\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.tol = tol\n",
    "        self.subsample = subsample\n",
    "        self.trees = []\n",
    "        self.base_pred = None\n",
    "        self.error_progression = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the Gradient Boosting Tree model to the data.\n",
    "        Args: X (np.array): Feature matrix. |  y (np.array): Target labels.\n",
    "        Returns: BoostingTreeResults: Object containing the fitted model information.\n",
    "        \"\"\"\n",
    "        X = np.nan_to_num(X, nan=0.0)\n",
    "        y = np.nan_to_num(y, nan=0.0)\n",
    "        # Ensure X is at least 2D\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        # Normalize data\n",
    "        X_mean = X.mean(axis=0)\n",
    "        X_std = X.std(axis=0)\n",
    "        X = (X - X_mean) / (X_std + 1e-8)\n",
    "        self.base_pred = np.mean(y)\n",
    "        y_pred = np.full(y.shape, self.base_pred)\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        for i in range(self.num_trees):\n",
    "            # Compute residuals\n",
    "            residuals = y - y_pred\n",
    "\n",
    "            # Subsample the data (if subsample < 1.0)\n",
    "            if self.subsample < 1.0:\n",
    "                idx = np.random.choice(n_samples, int(n_samples * self.subsample), replace=False)\n",
    "                X_sub, residuals_sub = X[idx], residuals[idx]\n",
    "            else:\n",
    "                X_sub, residuals_sub = X, residuals\n",
    "\n",
    "            # Fit a new tree on residuals\n",
    "            tree = DecisionTreeRegressorCustom(max_depth=self.max_depth)\n",
    "            tree.fit(X_sub, residuals_sub)\n",
    "            tree_pred = tree.predict(X)\n",
    "\n",
    "            # Update predictions with scaled tree predictions\n",
    "            y_pred += self.learning_rate * tree_pred\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            # Calculate Mean Squared Error\n",
    "            mse = MyMSE.calculate(y, y_pred)\n",
    "            self.error_progression.append(mse)\n",
    "\n",
    "            # Stop training if the tolerance is reached\n",
    "            if mse < self.tol:\n",
    "                print(f\"Converged after {i + 1} trees\")\n",
    "                break\n",
    "\n",
    "        return BoostingTreeResults(self.base_pred, self.trees, self.learning_rate)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the target values for a given input matrix X.\n",
    "        Args:X (np.array): Input feature matrix.\n",
    "        Returns: np.array: Predicted target values.\n",
    "    \"\"\"\n",
    "        X = np.nan_to_num(X, nan=0.0) # Replace NaN values in X with 0.0\n",
    "        if X.ndim == 1: # If X is a 1-dimensional array, reshape it to a 2D array with one column\n",
    "            X = X.reshape(-1, 1)\n",
    "        # Calculate the mean and standard deviation of X along the columns\n",
    "        X_mean = X.mean(axis=0)\n",
    "        X_std = X.std(axis=0)\n",
    "        # Standardize X by subtracting the mean and dividing by the standard deviation\n",
    "        # Adding a small constant (1e-8) to the denominator to prevent division by zero\n",
    "        X = (X - X_mean) / (X_std + 1e-8)\n",
    "        # Initialize the predictions array with the base prediction value for each sample\n",
    "        y_pred = np.full(X.shape[0], self.base_pred)\n",
    "        # Iterate through each tree in the ensemble\n",
    "        for tree in self.trees:\n",
    "            # Update the predictions by adding the scaled predictions from each tree\n",
    "            y_pred += self.learning_rate * tree.predict(X) \n",
    "        return y_pred\n",
    "\n",
    "\n",
    "class BoostingTreeResults:\n",
    "    \"\"\"\n",
    "        Initializes a BoostingTreeResults object.\n",
    "        Args:base_pred (float): The base prediction value.\n",
    "            trees (list): A list of DecisionTreeRegressorCustom objects.\n",
    "            learning_rate (float): The learning rate for updating predictions.\n",
    "        \"\"\"\n",
    "    def __init__(self, base_pred, trees, learning_rate):\n",
    "        self.base_pred = base_pred # Store the base prediction value\n",
    "        self.trees = trees # Store the list of trees in the ensemble\n",
    "        self.learning_rate = learning_rate # Store the learning rate for updates\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Method to make predictions based on input features X\n",
    "        Args: X (np.array): Input feature matrix.\n",
    "            Returns: np.array: Predicted target values.\n",
    "        \"\"\"\n",
    "        X = np.nan_to_num(X, nan=0.0) # Replace NaN values in X with 0.0\n",
    "        if X.ndim == 1: # If X is a 1-dimensional array, reshape it to a 2D array with one column\n",
    "            X = X.reshape(-1, 1)\n",
    "            \n",
    "        # Calculate the mean and standard deviation of X along the columns\n",
    "        X_mean = X.mean(axis=0)\n",
    "        X_std = X.std(axis=0)\n",
    "\n",
    "        # Standardize X by subtracting the mean and dividing by the standard deviation\n",
    "        # Adding a small constant (1e-8) to the denominator to prevent division by zero\n",
    "        X = (X - X_mean) / (X_std + 1e-8)\n",
    "\n",
    "        # Initialize the predictions array with the base prediction value for each sample\n",
    "        y_pred = np.full(X.shape[0], self.base_pred)\n",
    "\n",
    "        # Iterate through each tree in the ensemble\n",
    "        for tree in self.trees:\n",
    "            # Update the predictions by adding the scaled predictions from each tree\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "        return y_pred # Return the final predictions\n",
    "\n",
    "\n",
    "class DecisionTreeRegressorCustom:\n",
    "    \"\"\"\n",
    "    A decision tree regressor implementation.\n",
    "    Attributes: max_depth (int): The maximum depth of the tree.\n",
    "        min_samples_split (int): The minimum number of samples required to split Â  a node.\n",
    "        tree (dict): The tree structure, represented as a nested dictionary.\n",
    "    \"\"\"\n",
    "    # Initialize the DecisionTreeRegressorCustom with maximum depth and minimum samples required to split\n",
    "    def __init__(self, max_depth=3, min_samples_split=2):\n",
    "        self.max_depth = max_depth  # Maximum depth of the tree\n",
    "        self.min_samples_split = min_samples_split  # Minimum samples required to perform a split\n",
    "        self.tree = None  # Placeholder for the tree structure\n",
    "\n",
    "    # Fit the model to the training data (X, y)\n",
    "    def fit(self, X, y):\n",
    "        # Build the decision tree using the training data\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    # Predict the target values for the input data X\n",
    "    def predict(self, X):\n",
    "        # Traverse the tree for each sample in X and return the predictions as a numpy array\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "\n",
    "    # Traverse the decision tree to find the prediction for a single sample x\n",
    "    def _traverse_tree(self, x, node):\n",
    "        # If the current node is a leaf node, return its value\n",
    "        if \"value\" in node:\n",
    "            return node[\"value\"]\n",
    "\n",
    "        # Get the feature value for the current node's feature index\n",
    "        feature_value = x[node[\"feature_index\"]]\n",
    "\n",
    "        # Decide whether to traverse left or right based on the threshold\n",
    "        if feature_value <= node[\"threshold\"]:\n",
    "            return self._traverse_tree(x, node[\"left\"])  # Traverse left branch\n",
    "        else:\n",
    "            return self._traverse_tree(x, node[\"right\"])  # Traverse right branch\n",
    "\n",
    "    # Build the decision tree recursively\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        # Check stopping criteria: max depth, minimum samples, or no variance in y\n",
    "        if depth >= self.max_depth or len(y) < self.min_samples_split or np.var(y) == 0:\n",
    "            return {\"value\": np.mean(y)}  # Return leaf node with mean value\n",
    "\n",
    "        # Find the best feature and threshold to split on\n",
    "        feature_index, threshold = self._find_best_split(X, y)\n",
    "        if feature_index is None:\n",
    "            return {\"value\": np.mean(y)}  # Return leaf node if no valid split found\n",
    "\n",
    "        # Create boolean arrays for left and right splits based on the threshold\n",
    "        left_indices = X[:, feature_index] <= threshold\n",
    "        right_indices = X[:, feature_index] > threshold\n",
    "\n",
    "        # Recursively build the left and right branches of the tree\n",
    "        left_branch = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_branch = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        # Return the current node with feature index, threshold, and branches\n",
    "        return {\n",
    "            \"feature_index\": feature_index,\n",
    "            \"threshold\": threshold,\n",
    "            \"left\": left_branch,\n",
    "            \"right\": right_branch,\n",
    "        }\n",
    "\n",
    "    # \n",
    "    def _find_best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Find the best feature and threshold to split the data based on mean squared error reduction\n",
    "        Args: X (np.array): The training features. |   y (np.array): The training targets.\n",
    "        Returns: tuple: The best feature index and threshold.\n",
    "        \"\"\"\n",
    "        best_split = None  # Initialize the best split\n",
    "        best_mse_reduction = 0  # Initialize the best mean squared error reduction\n",
    "        current_mse = np.var(y) * len(y)  # Calculate the current mean squared error\n",
    "\n",
    "        # Iterate over each feature to find the best split\n",
    "        for feature_index in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature_index])  # Unique values of the feature\n",
    "            for threshold in thresholds:\n",
    "                # Create boolean arrays for left and right splits\n",
    "                left_indices = X[:, feature_index] <= threshold\n",
    "                right_indices = X[:, feature_index] > threshold\n",
    "\n",
    "                # Skip if any split results in an empty set\n",
    "                if len(y[left_indices]) == 0 or len(y[right_indices]) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Calculate mean squared error for left and right splits\n",
    "                mse_left = np.var(y[left_indices]) * len(left_indices)\n",
    "                mse_right = np.var(y[right_indices]) * len(right_indices)\n",
    "                mse_split = mse_left + mse_right  # Total mean squared error after the split\n",
    "\n",
    "                # Calculate the reduction in mean squared error\n",
    "                mse_reduction = current_mse - mse_split\n",
    "                # Update the best split if this one is better\n",
    "                if mse_reduction > best_mse_reduction:\n",
    "                    best_mse_reduction = mse_reduction\n",
    "                    best_split = (feature_index, threshold)\n",
    "\n",
    "        # Return the best split found, or (None, None) if no valid split exists\n",
    "        return best_split if best_split else (None, None)"
   ],
   "id": "2d907ce6b778d2ed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
