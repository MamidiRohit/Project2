{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444384f4-beab-41dd-aca5-38c04aa343c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ModelSelection:\n",
    "    def __init__(self, model, loss_function):\n",
    "        \"\"\"\n",
    "        Initialize the model selector with a given model and loss function.\n",
    "\n",
    "        Parameters:\n",
    "        - model: A class with `fit` and `predict` methods.\n",
    "        - loss_function: A callable that takes (y_true, y_pred) and returns a scalar loss.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "    def k_fold_cross_validation(self, X, y, k=5):\n",
    "        \"\"\"\n",
    "        Perform k-fold cross-validation.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Feature matrix (numpy array).\n",
    "        - y: Target vector (numpy array).\n",
    "        - k: Number of folds (default is 5).\n",
    "\n",
    "        Returns:\n",
    "        - mean_loss: The average loss across all folds.\n",
    "        \"\"\"\n",
    "        n = len(y)\n",
    "        indices = np.arange(n)\n",
    "        np.random.shuffle(indices)\n",
    "        fold_size = n // k\n",
    "        losses = []\n",
    "\n",
    "        for i in range(k):\n",
    "            test_indices = indices[i * fold_size:(i + 1) * fold_size]\n",
    "            train_indices = np.setdiff1d(indices, test_indices)\n",
    "\n",
    "            X_train, X_test = X[train_indices], X[test_indices]\n",
    "            y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "            self.model.fit(X_train, y_train)\n",
    "            y_pred = self.model.predict(X_test)\n",
    "            loss = self.loss_function(y_test, y_pred)\n",
    "            losses.append(loss)\n",
    "\n",
    "        mean_loss = np.mean(losses)\n",
    "        return mean_loss\n",
    "\n",
    "    def bootstrap(self, X, y, B=100):\n",
    "        \"\"\"\n",
    "        Perform bootstrap resampling to estimate prediction error.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Feature matrix (numpy array).\n",
    "        - y: Target vector (numpy array).\n",
    "        - B: Number of bootstrap samples (default is 100).\n",
    "\n",
    "        Returns:\n",
    "        - mean_loss: The average loss across all bootstrap samples.\n",
    "        \"\"\"\n",
    "        n = len(y)\n",
    "        losses = []\n",
    "\n",
    "        for _ in range(B):\n",
    "            bootstrap_indices = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "            oob_indices = np.setdiff1d(np.arange(n), bootstrap_indices)\n",
    "\n",
    "            if len(oob_indices) == 0:  # Skip iteration if no OOB samples\n",
    "                continue\n",
    "\n",
    "            X_train, X_test = X[bootstrap_indices], X[oob_indices]\n",
    "            y_train, y_test = y[bootstrap_indices], y[oob_indices]\n",
    "\n",
    "            self.model.fit(X_train, y_train)\n",
    "            y_pred = self.model.predict(X_test)\n",
    "            loss = self.loss_function(y_test, y_pred)\n",
    "            losses.append(loss)\n",
    "\n",
    "        mean_loss = np.mean(losses)\n",
    "        return mean_loss\n",
    "\n",
    "    def evaluate_model(self, X, y, method='k_fold', **kwargs):\n",
    "        \"\"\"\n",
    "        Evaluate the model using the specified method.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Feature matrix (numpy array).\n",
    "        - y: Target vector (numpy array).\n",
    "        - method: 'k_fold' or 'bootstrap'.\n",
    "        - kwargs: Additional parameters for the evaluation method.\n",
    "\n",
    "        Returns:\n",
    "        - loss: The evaluation loss.\n",
    "        \"\"\"\n",
    "        if method == 'k_fold':\n",
    "            return self.k_fold_cross_validation(X, y, **kwargs)\n",
    "        elif method == 'bootstrap':\n",
    "            return self.bootstrap(X, y, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported method. Choose 'k_fold' or 'bootstrap'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e08ac538-b003-4a57-b172-def7134d705e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Fold Cross-Validation Loss: 0.010218721629392193\n",
      "Bootstrap Loss: 0.010187779954984457\n",
      "Mean Squared Error: 0.0093\n",
      "First 10 Predictions: [-6.02845904e-01  7.50176355e-01 -1.04552162e+00  2.03622865e+00\n",
      "  1.01601727e+00  2.05011940e-01  6.97539978e-01 -1.39914961e-03\n",
      " -6.96849252e-01 -3.76952506e-01]\n"
     ]
    }
   ],
   "source": [
    "# Example of a simple linear regression model\n",
    "class SimpleLinearModel:\n",
    "    def fit(self, X, y):\n",
    "        self.coef_ = np.linalg.pinv(X) @ y\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.coef_\n",
    "\n",
    "# Mean squared error loss function\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Create synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 3)\n",
    "y = X @ np.array([1.5, -2.0, 1.0]) + np.random.randn(100) * 0.1\n",
    "\n",
    "# Initialize model and model selector\n",
    "model = SimpleLinearModel()\n",
    "selector = ModelSelection(model, mean_squared_error)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "k_fold_loss = selector.evaluate_model(X, y, method='k_fold', k=5)\n",
    "print(\"K-Fold Cross-Validation Loss:\", k_fold_loss)\n",
    "\n",
    "# Perform bootstrap\n",
    "bootstrap_loss = selector.evaluate_model(X, y, method='bootstrap', B=100)\n",
    "print(\"Bootstrap Loss:\", bootstrap_loss)\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "# Evaluate\n",
    "predictions = model.predict(X)\n",
    "mse = np.mean((y - predictions) ** 2)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(\"First 10 Predictions:\", predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12b7df6c-efcb-4966-8a5d-f0dbdf3ec6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 21.8948\n",
      "First 10 Predictions: [30.00384338 25.02556238 30.56759672 28.60703649 27.94352423 25.25628446\n",
      " 23.00180827 19.53598843 11.52363685 18.92026211]\n"
     ]
    }
   ],
   "source": [
    "#Dataset1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Features and Target\n",
    "X = data.drop('medv', axis=1).values\n",
    "y = data['medv'].values\n",
    "\n",
    "# Train Linear Regression (First Principles)\n",
    "class LinearRegression:\n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n",
    "        self.coef_ = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n",
    "        return X @ self.coef_\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Evaluate\n",
    "predictions = model.predict(X)\n",
    "mse = np.mean((y - predictions) ** 2)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(\"First 10 Predictions:\", predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f2f344f-9c22-4413-9840-b82461cc1df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9648\n",
      "First 10 Predictions: [6 9 3 7 2 2 5 2 5 2]\n",
      "Actual Labels:         [6 9 3 7 2 1 5 2 5 2]\n"
     ]
    }
   ],
   "source": [
    "# Dataset 2: Multi-Class Logistic Regression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Digits dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target  # Features and target\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Multi-class Logistic Regression with Softmax\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, epochs=5000):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.coefficients = None\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Prevent overflow\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def fit(self, X, y, num_classes):\n",
    "        # Add bias term\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        self.coefficients = np.random.randn(num_classes, X.shape[1]) * 0.01  # Random initialization\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            logits = X @ self.coefficients.T  # Linear combination\n",
    "            probabilities = self._softmax(logits)  # Apply softmax activation\n",
    "            \n",
    "            # Create one-hot encoded target matrix\n",
    "            y_one_hot = np.zeros((y.size, num_classes))\n",
    "            y_one_hot[np.arange(y.size), y] = 1\n",
    "            \n",
    "            # Compute gradient\n",
    "            gradient = X.T @ (probabilities - y_one_hot) / len(y)\n",
    "            self.coefficients -= self.lr * gradient.T  # Update coefficients\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Add bias term\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        logits = X @ self.coefficients.T\n",
    "        probabilities = self._softmax(logits)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LogisticRegression(lr=0.01, epochs=5000)\n",
    "num_classes = len(np.unique(y_train))\n",
    "model.fit(X_train, y_train, num_classes)\n",
    "\n",
    "# Evaluate\n",
    "predictions = model.predict(X_test)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"First 10 Predictions:\", predictions[:10])\n",
    "print(\"Actual Labels:        \", y_test[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9018a638-6c93-4396-8b45-5653aa333b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6536\n",
      "First 10 Predictions: [0 0 0 0 1 0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "#Dataset3\n",
    "# Load dataset\n",
    "file_path = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigree', 'Age', 'Outcome']\n",
    "data = pd.read_csv(file_path, names=columns)\n",
    "\n",
    "# Features and Target\n",
    "X = data.drop('Outcome', axis=1).values\n",
    "y = data['Outcome'].values\n",
    "\n",
    "# Train Perceptron (Binary Classification)\n",
    "class Perceptron:\n",
    "    def __init__(self, lr=0.1, epochs=100):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        for _ in range(self.epochs):\n",
    "            for i in range(len(y)):\n",
    "                prediction = 1 if X[i] @ self.weights > 0 else 0\n",
    "                self.weights += self.lr * (y[i] - prediction) * X[i]\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n",
    "        return (X @ self.weights > 0).astype(int)\n",
    "\n",
    "model = Perceptron(lr=0.1, epochs=100)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Evaluate\n",
    "predictions = model.predict(X)\n",
    "accuracy = np.mean(predictions == y)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"First 10 Predictions:\", predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99c4e2e2-25c0-471b-9120-9a0a61f56131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.4175\n",
      "First 10 Predictions: [5.04323344 5.13506106 5.21178615 5.68140344 5.04323344 5.07773597\n",
      " 5.1034784  5.32451785 5.32838568 5.63530842]\n"
     ]
    }
   ],
   "source": [
    "#Dataset4\n",
    "# Load dataset\n",
    "file_path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "data = pd.read_csv(file_path, sep=';')\n",
    "\n",
    "# Features and Target\n",
    "X = data.drop('quality', axis=1).values\n",
    "y = data['quality'].values\n",
    "\n",
    "# Train Ridge Regression (First Principles)\n",
    "class RidgeRegression:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n",
    "        I = np.eye(X.shape[1])\n",
    "        I[0, 0] = 0  # Do not regularize bias term\n",
    "        self.coef_ = np.linalg.pinv(X.T @ X + self.alpha * I) @ X.T @ y\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n",
    "        return X @ self.coef_\n",
    "\n",
    "model = RidgeRegression(alpha=1.0)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Evaluate\n",
    "predictions = model.predict(X)\n",
    "mse = np.mean((y - predictions) ** 2)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(\"First 10 Predictions:\", predictions[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ae2b8f-39ae-473f-acb5-26e54f71f352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
