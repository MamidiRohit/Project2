{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5c3dbe-a79d-404d-9422-8659dd4a7584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 2: Multi-Class Logistic Regression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Digits dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target  # Features and target\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Multi-class Logistic Regression with Softmax\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, epochs=5000):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.coefficients = None\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Prevent overflow\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def fit(self, X, y, num_classes):\n",
    "        # Add bias term\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        self.coefficients = np.random.randn(num_classes, X.shape[1]) * 0.01  # Random initialization\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            logits = X @ self.coefficients.T  # Linear combination\n",
    "            probabilities = self._softmax(logits)  # Apply softmax activation\n",
    "            \n",
    "            # Create one-hot encoded target matrix\n",
    "            y_one_hot = np.zeros((y.size, num_classes))\n",
    "            y_one_hot[np.arange(y.size), y] = 1\n",
    "            \n",
    "            # Compute gradient\n",
    "            gradient = X.T @ (probabilities - y_one_hot) / len(y)\n",
    "            self.coefficients -= self.lr * gradient.T  # Update coefficients\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Add bias term\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        logits = X @ self.coefficients.T\n",
    "        probabilities = self._softmax(logits)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LogisticRegression(lr=0.01, epochs=5000)\n",
    "num_classes = len(np.unique(y_train))\n",
    "model.fit(X_train, y_train, num_classes)\n",
    "\n",
    "# Evaluate\n",
    "predictions = model.predict(X_test)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"First 10 Predictions:\", predictions[:10])\n",
    "print(\"Actual Labels:        \", y_test[:10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
