# -*- coding: utf-8 -*-
"""gradientboostingregression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fxtGqQuDA0KXlc7gO_ve2DRTh9o3Mht5
"""

import numpy as np
from sklearn.tree import DecisionTreeRegressor
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import csv

class GradientBoostingTreeRegressor(BaseEstimator, RegressorMixin):
    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3, random_state=None):
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.max_depth = max_depth
        self.random_state = random_state

    def _initialize_f0(self, y):
        return np.mean(y)

    def _negative_gradient(self, y, pred):
        return y - pred

    def fit(self, X, y):
        X = np.asarray(X)
        y = np.asarray(y)

        self.f0 = self._initialize_f0(y)
        self.trees_ = []

        F = np.full(len(y), self.f0)
        np.random.seed(self.random_state)

        # Early stopping variables
        best_mse = float('inf')
        patience = 5
        patience_counter = 0

        for m in range(self.n_estimators):
            residuals = self._negative_gradient(y, F)

            tree = DecisionTreeRegressor(
                max_depth=self.max_depth,
                random_state=self.random_state
            )

            tree.fit(X, residuals)
            update = self.learning_rate * tree.predict(X)
            F += update
            self.trees_.append(tree)

            # Early stopping check
            current_mse = mean_squared_error(y, F)
            if current_mse < best_mse:
                best_mse = current_mse
                patience_counter = 0
            else:
                patience_counter += 1

            if patience_counter >= patience:
                print(f"Early stopping at iteration {m}")
                break

        return self

    def predict(self, X):
        X = np.asarray(X)
        pred = np.full(len(X), self.f0)
        for tree in self.trees_:
            pred += self.learning_rate * tree.predict(X)
        return pred

def load_data(filename):
    import csv
    import numpy as np
    from sklearn.preprocessing import LabelEncoder

    with open(filename, 'r') as f:
        reader = csv.reader(f)
        # Skip header if it exists
        # next(reader)  # Uncomment if you have headers

        # Initialize label encoders for categorical columns
        categorical_encoders = {}
        X, y = [], []

        for row in reader:
            features = []
            for i, val in enumerate(row[:-1]):  # All columns except the last
                try:
                    # Try converting to float first
                    features.append(float(val))
                except ValueError:
                    # If conversion fails, it's categorical data
                    if i not in categorical_encoders:
                        categorical_encoders[i] = LabelEncoder()
                    # Fit and transform the categorical value
                    encoded_val = categorical_encoders[i].fit_transform([val])[0]
                    features.append(encoded_val)

            # Handle target variable (last column)
            try:
                y_val = float(row[-1])
            except ValueError:
                if 'target_encoder' not in categorical_encoders:
                    categorical_encoders['target_encoder'] = LabelEncoder()
                y_val = categorical_encoders['target_encoder'].fit_transform([row[-1]])[0]

            X.append(features)
            y.append(y_val)

        return np.array(X), np.array(y)

def test_model(filename):
    # Load data
    X, y = load_data(filename)

    # Scale target values to [0,1]
    scaler = MinMaxScaler()
    y_scaled = scaler.fit_transform(y.reshape(-1, 1)).ravel()

    # Initialize and train model
    model = GradientBoostingTreeRegressor(
        n_estimators=600,
        learning_rate=0.1,
        max_depth=3,
        random_state=42
    )

    # Fit the model
    model.fit(X, y_scaled)

    # Make predictions
    predictions_scaled = model.predict(X)
    predictions = scaler.inverse_transform(predictions_scaled.reshape(-1, 1)).ravel()

    # Calculate metrics
    mse = mean_squared_error(y, predictions)
    r2 = r2_score(y, predictions)

    print("Gradient Boosting Results:")
    print(f"Mean Squared Error: {mse:.4f}")
    print(f"RÂ² Score: {r2:.4f}")

    # Visualization
    plt.figure(figsize=(10, 6))
    plt.scatter(y, predictions, alpha=0.5)
    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)
    plt.title('Gradient Boosting: Predicted vs Actual Values')
    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.grid(True)
    plt.show()

    return predictions, y

if __name__ == "__main__":
    predictions, actuals = test_model('mldata.csv')